{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marko\\anaconda3\\envs\\rllib-torch\\lib\\site-packages\\pmaw\\Request.py:57: UserWarning: Using safe_exit without setting until value is not recommended. Setting until to 1738709863\n",
      "  warnings.warn(\n",
      "2025-02-04 23:57:43,601 - INFO - Response cache key: a97ef3c62acdd2f30ea811f4a759655b\n",
      "2025-02-04 23:57:43,603 - INFO - No previous requests to load\n",
      "2025-02-04 23:57:45,706 - WARNING - Not all PushShift shards are active. Query results may be incomplete.\n",
      "2025-02-04 23:57:45,708 - INFO - 0 result(s) available in Pushshift\n",
      "2025-02-04 23:57:46,312 - ERROR - twitter API error 401: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "2025-02-04 23:57:47,997 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-04 23:57:48,963 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "c:\\Users\\marko\\anaconda3\\envs\\rllib-torch\\lib\\site-packages\\pmaw\\Request.py:57: UserWarning: Using safe_exit without setting until value is not recommended. Setting until to 1738709869\n",
      "  warnings.warn(\n",
      "2025-02-04 23:57:49,050 - INFO - Response cache key: 0218a8e39c6310784b9947fc023a9ca1\n",
      "2025-02-04 23:57:49,052 - INFO - No previous requests to load\n",
      "2025-02-04 23:57:50,835 - WARNING - Not all PushShift shards are active. Query results may be incomplete.\n",
      "2025-02-04 23:57:50,837 - INFO - 0 result(s) available in Pushshift\n",
      "2025-02-04 23:57:51,450 - ERROR - twitter API error 401: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "2025-02-04 23:57:52,669 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-04 23:57:53,441 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "c:\\Users\\marko\\anaconda3\\envs\\rllib-torch\\lib\\site-packages\\pmaw\\Request.py:57: UserWarning: Using safe_exit without setting until value is not recommended. Setting until to 1738709873\n",
      "  warnings.warn(\n",
      "2025-02-04 23:57:53,448 - INFO - Response cache key: ac6a25bd6d492426f362bafb5a9a57e0\n",
      "2025-02-04 23:57:53,451 - INFO - No previous requests to load\n",
      "2025-02-04 23:57:55,233 - WARNING - Not all PushShift shards are active. Query results may be incomplete.\n",
      "2025-02-04 23:57:55,234 - INFO - 0 result(s) available in Pushshift\n",
      "2025-02-04 23:57:55,839 - ERROR - twitter API error 401: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "2025-02-04 23:57:57,242 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-04 23:57:58,275 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "c:\\Users\\marko\\anaconda3\\envs\\rllib-torch\\lib\\site-packages\\pmaw\\Request.py:57: UserWarning: Using safe_exit without setting until value is not recommended. Setting until to 1738709878\n",
      "  warnings.warn(\n",
      "2025-02-04 23:57:58,281 - INFO - Response cache key: 069a6e5b42be233fd9a2af71c5c46380\n",
      "2025-02-04 23:57:58,284 - INFO - No previous requests to load\n",
      "2025-02-04 23:57:59,948 - WARNING - Not all PushShift shards are active. Query results may be incomplete.\n",
      "2025-02-04 23:57:59,951 - INFO - 0 result(s) available in Pushshift\n",
      "2025-02-04 23:58:00,600 - ERROR - twitter API error 401: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "2025-02-04 23:58:01,247 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-02-04 23:58:01,249 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import openai\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pmaw import PushshiftAPI\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class HistoricalCryptoAnalyzer:\n",
    "    def __init__(self, start_date, end_date, use_gpt_cleaning=True):\n",
    "        # Date validation and business day calculation\n",
    "        self.start_date, self.end_date = self._validate_dates(start_date, end_date)\n",
    "        self.business_days = pd.date_range(start=self.start_date, end=self.end_date, freq=BDay())\n",
    "        \n",
    "        # API configuration\n",
    "        self.api_config = {\n",
    "            \"santiment\": {\n",
    "                \"endpoint\": \"https://api.santiment.net/graphql\",\n",
    "                \"headers\": {\"Authorization\": f\"Bearer {os.getenv('SANTIMENT_KEY')}\"},\n",
    "                \"query\": \"\"\"\n",
    "                    query ($slug: String!, $from: DateTime!, $to: DateTime!) {\n",
    "                        getMetric(metric: \"sentiment_volume_consumed\") {\n",
    "                            timeseriesData(\n",
    "                                slug: $slug\n",
    "                                from: $from\n",
    "                                to: $to\n",
    "                                interval: \"1d\"\n",
    "                            ) { datetime, value }\n",
    "                        }\n",
    "                    }\n",
    "                \"\"\"\n",
    "            },\n",
    "            \"twitter\": {\n",
    "                \"endpoint\": \"https://api.twitter.com/2/tweets/search/all\",\n",
    "                \"headers\": {\"Authorization\": f\"Bearer {os.getenv('TWITTER_BEARER')}\"},\n",
    "                \"params_template\": {\n",
    "                    \"max_results\": 500,\n",
    "                    \"tweet.fields\": \"created_at,public_metrics,lang\",\n",
    "                    \"expansions\": \"author_id\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.cryptos = [\"BTC\", \"ETH\", \"BNB\", \"XRP\", \"ADA\", \"DOGE\"]\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.rate_limiter = RateLimiter()\n",
    "        self.use_gpt_cleaning = use_gpt_cleaning\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"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\")\n",
    "\n",
    "    def _validate_dates(self, start, end):\n",
    "        \"\"\"Enforce Santiment free tier limitations\"\"\"\n",
    "        start_date = pd.to_datetime(start)\n",
    "        end_date = pd.to_datetime(end)\n",
    "        \n",
    "        max_window = pd.Timedelta(days=730)  # 2 years\n",
    "        if end_date - start_date > max_window:\n",
    "            new_start = end_date - max_window\n",
    "            logging.warning(f\"Adjusting start date to {new_start} for Santiment free tier\")\n",
    "            start_date = new_start\n",
    "            \n",
    "        min_allowed_date = pd.Timestamp.now() - pd.Timedelta(days=730)\n",
    "        if start_date < min_allowed_date:\n",
    "            raise ValueError(\"Free tier only allows data up to 2 years old\")\n",
    "            \n",
    "        return start_date, end_date\n",
    "\n",
    "    def _fetch_santiment(self, crypto, date):\n",
    "        \"\"\"Santiment API with free tier constraints\"\"\"\n",
    "        self.rate_limiter.wait('santiment')\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_config[\"santiment\"][\"endpoint\"],\n",
    "                headers=self.api_config[\"santiment\"][\"headers\"],\n",
    "                json={\n",
    "                    \"query\": self.api_config[\"santiment\"][\"query\"],\n",
    "                    \"variables\": {\n",
    "                        \"slug\": crypto.lower(),\n",
    "                        \"from\": (date - BDay(1)).isoformat(),\n",
    "                        \"to\": date.isoformat()\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'errors' in data:\n",
    "                    logging.warning(f\"Santiment restrictions: {data['errors'][0]['message']}\")\n",
    "                    return []\n",
    "                return data['data']['getMetric']['timeseriesData']\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Santiment error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _fetch_reddit(self, crypto, date):\n",
    "        \"\"\"Pushshift API with PMAW optimization\"\"\"\n",
    "        self.rate_limiter.wait('pushshift')\n",
    "        try:\n",
    "            api = PushshiftAPI(\n",
    "                num_workers=10,\n",
    "                rate_limit=60,\n",
    "                limit_type='backoff',\n",
    "                jitter='decorr'\n",
    "            )\n",
    "            \n",
    "            start_ts = int((date - BDay(1)).timestamp())\n",
    "            end_ts = int(date.timestamp())\n",
    "            \n",
    "            posts = api.search_submissions(\n",
    "                q=crypto,\n",
    "                after=start_ts,\n",
    "                before=end_ts,\n",
    "                subreddit=\"CryptoCurrency,Bitcoin,ethereum\",\n",
    "                filter=['title', 'selftext', 'created_utc'],\n",
    "                mem_safe=True,\n",
    "                safe_exit=True\n",
    "            )\n",
    "            return [f\"{p['title']} {p['selftext']}\" for p in posts]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Pushshift error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _fetch_twitter(self, crypto, date):\n",
    "        \"\"\"Twitter Academic API with historical support\"\"\"\n",
    "        self.rate_limiter.wait('twitter')\n",
    "        try:\n",
    "            params = {\n",
    "                **self.api_config[\"twitter\"][\"params_template\"],\n",
    "                \"query\": f\"#{crypto} lang:en -is:retweet\",\n",
    "                \"start_time\": (date - BDay(1)).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "                \"end_time\": date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            }\n",
    "            \n",
    "            response = requests.get(\n",
    "                self.api_config[\"twitter\"][\"endpoint\"],\n",
    "                headers=self.api_config[\"twitter\"][\"headers\"],\n",
    "                params=params\n",
    "            )\n",
    "            \n",
    "            if handle_api_errors(response, 'twitter'):\n",
    "                return self._fetch_twitter(crypto, date)  # Retry\n",
    "            \n",
    "            return [tweet['text'] for tweet in response.json().get('data', [])]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Twitter error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _process_day(self, date):\n",
    "        \"\"\"Process data for a single business day\"\"\"\n",
    "        daily_data = []\n",
    "        \n",
    "        for crypto in self.cryptos:\n",
    "            try:\n",
    "                # Data collection\n",
    "                reddit = self._fetch_reddit(crypto, date)\n",
    "                twitter = self._fetch_twitter(crypto, date)\n",
    "                santiment = self._fetch_santiment(crypto, date)\n",
    "                \n",
    "                # Text processing\n",
    "                raw_text = \" \".join(reddit + twitter)\n",
    "                cleaned_text = self._clean_text(raw_text)\n",
    "                \n",
    "                # Feature generation\n",
    "                embedding = self._generate_embedding(cleaned_text)\n",
    "                sentiment = self._analyze_sentiment(cleaned_text)\n",
    "                \n",
    "                daily_data.append({\n",
    "                    \"date\": date.date(),\n",
    "                    \"crypto\": crypto,\n",
    "                    \"santiment_score\": santiment[0]['value'] if santiment else np.nan,\n",
    "                    \"embedding\": embedding,\n",
    "                    **sentiment\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed processing {crypto} on {date}: {e}\")\n",
    "        \n",
    "        return daily_data\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        if self.use_gpt_cleaning:\n",
    "            return self._gpt_clean(text)\n",
    "        return self._basic_clean(text)\n",
    "\n",
    "    def _gpt_clean(self, text):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Clean this crypto text, remove noise, preserve sentiment:\\n{text}\"\n",
    "                }],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message.content.lower().strip()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"GPT cleaning failed: {e}\")\n",
    "            return self._basic_clean(text)\n",
    "\n",
    "    def _basic_clean(self, text):\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[@#]\\w+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        doc = self.nlp(text.lower())\n",
    "        tokens = [\n",
    "            token.lemma_ for token in doc\n",
    "            if not token.is_stop \n",
    "            and not token.is_punct \n",
    "            and not token.like_num\n",
    "            and len(token.text) > 2\n",
    "        ]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def _generate_embedding(self, text):\n",
    "        if not text:\n",
    "            return np.nan\n",
    "        try:\n",
    "            response = openai.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=text,\n",
    "                dimensions=1536\n",
    "            )\n",
    "            return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Embedding failed: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    def _analyze_sentiment(self, text):\n",
    "        return {\n",
    "            \"vader\": self.vader.polarity_scores(text)['compound'],\n",
    "            \"textblob\": TextBlob(text).sentiment.polarity,\n",
    "            \"word_count\": len(text.split())\n",
    "        }\n",
    "\n",
    "    def execute_backtest(self):\n",
    "        master_df = pd.DataFrame()\n",
    "        \n",
    "        for idx, date in enumerate(self.business_days):\n",
    "            if date.weekday() < 5:  # Ensure business day\n",
    "                daily_data = self._process_day(date)\n",
    "                master_df = pd.concat([master_df, pd.DataFrame(daily_data)], ignore_index=True)\n",
    "                \n",
    "                # Save checkpoint every week\n",
    "                if idx % 5 == 0:\n",
    "                    master_df.to_parquet(f\"backtest_checkpoint_{date.date()}.parquet\")\n",
    "        \n",
    "        # Final processing and return\n",
    "        master_df['date'] = pd.to_datetime(master_df['date'])\n",
    "        master_df.set_index(['date', 'crypto'], inplace=True)\n",
    "        return master_df\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Enhanced rate limiting system\"\"\"\n",
    "    def __init__(self):\n",
    "        self.buckets = {\n",
    "            'santiment': TokenBucket(60, 60),  # 60 requests/minute\n",
    "            'pushshift': TokenBucket(60, 60),\n",
    "            'twitter': TokenBucket(300, 900)   # 300 requests/15 minutes\n",
    "        }\n",
    "\n",
    "    def wait(self, service):\n",
    "        while not self.buckets[service].consume(1):\n",
    "            time.sleep(0.1)\n",
    "\n",
    "class TokenBucket:\n",
    "    \"\"\"Token bucket rate limiting implementation\"\"\"\n",
    "    def __init__(self, capacity, refill_period):\n",
    "        self.capacity = capacity\n",
    "        self.tokens = capacity\n",
    "        self.refill_period = refill_period\n",
    "        self.last_refill = time.time()\n",
    "\n",
    "    def consume(self, tokens=1):\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_refill\n",
    "        \n",
    "        if elapsed > self.refill_period:\n",
    "            self.tokens = self.capacity\n",
    "            self.last_refill = now\n",
    "            \n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def handle_api_errors(response, service):\n",
    "    \"\"\"Handle API-specific error codes\"\"\"\n",
    "    if response.status_code == 429:\n",
    "        retry_after = int(response.headers.get('Retry-After', 60))\n",
    "        logging.warning(f\"Rate limited on {service}. Retrying in {retry_after}s\")\n",
    "        time.sleep(retry_after)\n",
    "        return True\n",
    "    elif response.status_code >= 400:\n",
    "        logging.error(f\"{service} API error {response.status_code}: {response.text}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = HistoricalCryptoAnalyzer(\n",
    "        start_date=\"2024-02-03\",\n",
    "        end_date=\"2025-02-03\",\n",
    "        use_gpt_cleaning=True\n",
    "    )\n",
    "    \n",
    "    features_df = analyzer.execute_backtest()\n",
    "    features_df.to_parquet(\"crypto_sentiment_backtest.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
