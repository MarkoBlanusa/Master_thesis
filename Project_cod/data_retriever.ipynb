{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = {\n",
    "    \"US_Stocks\": \"^GSPC\",\n",
    "    \"Global_Stocks\": \"URTH\",\n",
    "    \"Emerging_Markets\": \"EEM\",\n",
    "    \"US_Bonds\": \"AGG\",\n",
    "    \"Intl_Bonds\": \"BNDX\", \n",
    "    \"Broad_Commodities\": \"GSG\",\n",
    "    \"Gold\": \"GLD\",\n",
    "    \"US_RealEstate\": \"VNQ\",\n",
    "    \"Global_RealEstate\": \"VNQI\",\n",
    "    \"Cash\": \"^IRX\",\n",
    "    \"Bitcoin\": \"BTC-USD\",\n",
    "    \"Volatility_Index\": \"^VIX\",\n",
    "    \"Etherium\": \"ETH-USD\",\n",
    "    \"Inflation_Protected\": \"TIP\",\n",
    "    \"Private_Equity\": \"PSP\"\n",
    "}\n",
    "\n",
    "start_date = \"1970-01-01\"  # Try a more recent start date if older data isn't available\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "data = {}\n",
    "for name, ticker in tickers.items():\n",
    "    print(f\"Downloading data for {name} ({ticker})...\")\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "    print(df.head())\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No data for {name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # If multi-indexed columns, flatten them\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Attempt to drop levels if they are consistent. For example, \n",
    "        # if you see columns like (Price, ^GSPC, Adj Close), dropping two levels:\n",
    "        df.columns = df.columns.droplevel([1])\n",
    "\n",
    "    print(f\"Columns after flattening for {name}:\", df.columns)\n",
    "\n",
    "    if 'Adj Close' in df.columns:\n",
    "        data[name] = df['Adj Close']\n",
    "    else:\n",
    "        print(f\"'Adj Close' not found in {name}. Available columns: {df.columns}\")\n",
    "\n",
    "if data:\n",
    "    combined_data = pd.DataFrame(data)\n",
    "    combined_data.index.name = \"Date\"\n",
    "    combined_data = combined_data.dropna()\n",
    "    combined_data.to_csv(\"Diversified_Portfolio_Data_Complete.csv\")\n",
    "    print(\"Data saved to 'Diversified_Portfolio_Data_Complete.csv'\")\n",
    "    print(combined_data.head())\n",
    "else:\n",
    "    print(\"No valid data collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = {\n",
    "    # Equities\n",
    "    \"US_Stocks\": \"^GSPC\",     # S&P 500\n",
    "    # \"Dow_Jones\": \"^DJI\",      # Dow Jones Industrial Average, data back to 1920s\n",
    "    \"Nasdaq_Composite\": \"^IXIC\",\n",
    "\n",
    "    # \"Volatility_Index\": \"^VIX\",\n",
    "    \n",
    "    # Rates/Yields\n",
    "    \"3M_TBill\": \"^IRX\",       # 13-week T-bill\n",
    "    \"10Y_Treasury_Yield\": \"^TNX\",   # 10-year treasury yield (often starts ~1962)\n",
    "    \"30Y_Treasury_Yield\": \"^TYX\",   # 30-year treasury yield (often starts ~1977)\n",
    "    \n",
    "    # Commodities (continuous futures)\n",
    "    \"Gold\": \"^XAU\",   # COMEX Gold continuous contract, data sometimes back to 1970s\n",
    "    # \"Silver\": \"^DJCISI\",\n",
    "    \"Crude_Oil\": \"^XOI\", # NYMEX WTI Crude continuous contract, data often from early 1980s\n",
    "    \n",
    "    # Currency / Dollar Index\n",
    "    \"US_Dollar_Index\": \"^NYICDX\", # ICE U.S. Dollar Index futures, often from mid-1980s\n",
    "    \"Cash\": \"^IRX\",\n",
    "}\n",
    "\n",
    "start_date = \"1970-01-01\"  # Try a more recent start date if older data isn't available\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "data = {}\n",
    "for name, ticker in tickers.items():\n",
    "    print(f\"Downloading data for {name} ({ticker})...\")\n",
    "    df = yf.download(ticker, period=\"max\", interval=\"1d\")\n",
    "    print(df.head())\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No data for {name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # If multi-indexed columns, flatten them\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Attempt to drop levels if they are consistent. For example, \n",
    "        # if you see columns like (Price, ^GSPC, Adj Close), dropping two levels:\n",
    "        df.columns = df.columns.droplevel([1])\n",
    "\n",
    "    print(f\"Columns after flattening for {name}:\", df.columns)\n",
    "\n",
    "    if 'Adj Close' in df.columns:\n",
    "        data[name] = df['Adj Close']\n",
    "    else:\n",
    "        print(f\"'Adj Close' not found in {name}. Available columns: {df.columns}\")\n",
    "\n",
    "if data:\n",
    "    combined_data = pd.DataFrame(data)\n",
    "    combined_data.index.name = \"Date\"\n",
    "    combined_data = combined_data.dropna()\n",
    "    combined_data.to_csv(\"Diversified_Portfolio_Data_Complete.csv\")\n",
    "    print(\"Data saved to 'Diversified_Portfolio_Data_Complete.csv'\")\n",
    "    print(combined_data.head())\n",
    "else:\n",
    "    print(\"No valid data collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = {\n",
    "    \"Tron\": \"TRX-USD\",\n",
    "    \"Solana\": \"SOL-USD\",\n",
    "    \"Avalanche\": \"AVAX-USD\",\n",
    "    \"Xrp\": \"XRP-USD\",\n",
    "    \"Bnb\": \"BNB-USD\",\n",
    "    \"Bitcoin\": \"BTC-USD\",\n",
    "    \"Doge\": \"DOGE-USD\",\n",
    "    \"Etherium\": \"ETH-USD\",\n",
    "    \"PolkaDot\": \"DOT-USD\", \n",
    "    # \"Stellar\": \"XLM-USD\",\n",
    "    # \"Monero\": \"XMR-USD\", \n",
    "    # \"Neo\": \"NEO-USD\",\n",
    "    # \"Litecoin\": \"LTC-USD\",\n",
    "    # \"Cardano\": \"ADA-USD\",\n",
    "    # \"Eos\": \"EOS-USD\",\n",
    "\n",
    "    \"US_Stocks\": \"^GSPC\",\n",
    "    \"Global_Stocks\": \"URTH\",\n",
    "    \"Emerging_Markets\": \"EEM\",\n",
    "    \"US_Bonds\": \"AGG\",\n",
    "    \"Intl_Bonds\": \"BNDX\",\n",
    "    \"Broad_Commodities\": \"GSG\",\n",
    "    \"Gold\": \"GLD\",\n",
    "    \"US_RealEstate\": \"VNQ\",\n",
    "    \"Global_RealEstate\": \"VNQI\",\n",
    "    \"Cash\": \"^IRX\",\n",
    "    \"Volatility_Index\": \"^VIX\",\n",
    "    # \"CVIX\": \"CVOL-USD\",\n",
    "    \"Inflation_Protected\": \"TIP\",\n",
    "    \"Private_Equity\": \"PSP\",\n",
    "    \"DollarIndex\": \"^DXY\",   # US Dollar Index\n",
    "\n",
    "    # \"Apple\": \"AAPL\",\n",
    "    # \"Microsoft\": \"MSFT\",\n",
    "    # \"Amazon\": \"AMZN\",\n",
    "    # \"Tesla\": \"TSLA\",\n",
    "    # \"Google\": \"GOOGL\",\n",
    "    # \"Meta\": \"META\",\n",
    "    # \"NVIDIA\": \"NVDA\",\n",
    "    # \"ExxonMobil\": \"XOM\",\n",
    "    # \"JPMorgan\": \"JPM\",\n",
    "    # \"Johnson_Johnson\": \"JNJ\",\n",
    "    # \"Procter_Gamble\": \"PG\",\n",
    "    # \"Pfizer\": \"PFE\",\n",
    "    # \"CocaCola\": \"KO\",\n",
    "    # \"PepsiCo\": \"PEP\",\n",
    "    # \"Intel\": \"INTC\",\n",
    "    # \"Cisco\": \"CSCO\",\n",
    "    # \"Chevron\": \"CVX\",\n",
    "    # \"Goldman_Sachs\": \"GS\",\n",
    "    # \"Boeing\": \"BA\",\n",
    "    # \"Disney\": \"DIS\",\n",
    "    # \"US_Stocks\": \"^GSPC\",\n",
    "    # \"Global_Stocks\": \"URTH\",\n",
    "    # \"Emerging_Markets\": \"EEM\",\n",
    "    # # \"US_Bonds\": \"AGG\",\n",
    "    # # \"Intl_Bonds\": \"BNDX\",\n",
    "    # \"Broad_Commodities\": \"GSG\",\n",
    "    # \"Gold\": \"GLD\",\n",
    "    # \"US_RealEstate\": \"VNQ\",\n",
    "    # \"Global_RealEstate\": \"VNQI\",\n",
    "    # # \"Cash\": \"^IRX\",\n",
    "    # \"Volatility_Index\": \"^VIX\",\n",
    "    # \"Inflation_Protected\": \"TIP\",\n",
    "    # \"Private_Equity\": \"PSP\"\n",
    "}\n",
    "\n",
    "start_date = \"2000-01-01\"  # Try a more recent start date if older data isn't available\n",
    "end_date = \"2025-01-27\"\n",
    "\n",
    "data = {}\n",
    "for name, ticker in tickers.items():\n",
    "    print(f\"Downloading data for {name} ({ticker})...\")\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "    print(df.head())\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No data for {name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # If multi-indexed columns, flatten them\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Attempt to drop levels if they are consistent. For example, \n",
    "        # if you see columns like (Price, ^GSPC, Adj Close), dropping two levels:\n",
    "        df.columns = df.columns.droplevel([1])\n",
    "\n",
    "    print(f\"Columns after flattening for {name}:\", df.columns)\n",
    "\n",
    "    if 'Adj Close' in df.columns:\n",
    "        data[name] = df['Adj Close']\n",
    "    else:\n",
    "        print(f\"'Adj Close' not found in {name}. Available columns: {df.columns}\")\n",
    "\n",
    "if data:\n",
    "    combined_data = pd.DataFrame(data)\n",
    "    combined_data.index.name = \"Date\"\n",
    "    combined_data = combined_data.dropna()\n",
    "    combined_data.to_csv(\"Diversified_Portfolio_Data_Complete.csv\")\n",
    "    print(\"Data saved to 'Diversified_Portfolio_Data_Complete.csv'\")\n",
    "    print(combined_data.head())\n",
    "else:\n",
    "    print(\"No valid data collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['URTH']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['EEM']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BNDX']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['GSG']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['GLD']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for Global_Stocks (URTH)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, URTH), (Close, URTH), (High, URTH), (Low, URTH), (Open, URTH), (Volume, URTH)]\n",
      "Index: []\n",
      "No data for Global_Stocks. Skipping.\n",
      "Downloading data for Emerging_Markets (EEM)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, EEM), (Close, EEM), (High, EEM), (Low, EEM), (Open, EEM), (Volume, EEM)]\n",
      "Index: []\n",
      "No data for Emerging_Markets. Skipping.\n",
      "Downloading data for Intl_Bonds (BNDX)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, BNDX), (Close, BNDX), (High, BNDX), (Low, BNDX), (Open, BNDX), (Volume, BNDX)]\n",
      "Index: []\n",
      "No data for Intl_Bonds. Skipping.\n",
      "Downloading data for Broad_Commodities (GSG)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, GSG), (Close, GSG), (High, GSG), (Low, GSG), (Open, GSG), (Volume, GSG)]\n",
      "Index: []\n",
      "No data for Broad_Commodities. Skipping.\n",
      "Downloading data for Gold (GLD)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, GLD), (Close, GLD), (High, GLD), (Low, GLD), (Open, GLD), (Volume, GLD)]\n",
      "Index: []\n",
      "No data for Gold. Skipping.\n",
      "Downloading data for Global_RealEstate (VNQI)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1 Failed download:\n",
      "['VNQI']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['^IRX']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['^VIX']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [(Adj Close, VNQI), (Close, VNQI), (High, VNQI), (Low, VNQI), (Open, VNQI), (Volume, VNQI)]\n",
      "Index: []\n",
      "No data for Global_RealEstate. Skipping.\n",
      "Downloading data for Cash (^IRX)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, ^IRX), (Close, ^IRX), (High, ^IRX), (Low, ^IRX), (Open, ^IRX), (Volume, ^IRX)]\n",
      "Index: []\n",
      "No data for Cash. Skipping.\n",
      "Downloading data for Volatility_Index (^VIX)...\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, ^VIX), (Close, ^VIX), (High, ^VIX), (Low, ^VIX), (Open, ^VIX), (Volume, ^VIX)]\n",
      "Index: []\n",
      "No data for Volatility_Index. Skipping.\n",
      "No valid data collected.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = {\n",
    "    # \"Tron\": \"TRX-USD\",\n",
    "    # \"Solana\": \"SOL-USD\",\n",
    "    # \"Avalanche\": \"AVAX-USD\",\n",
    "    # \"Xrp\": \"XRP-USD\",\n",
    "    # \"Bnb\": \"BNB-USD\",\n",
    "    # \"Bitcoin\": \"BTC-USD\",\n",
    "    # \"Doge\": \"DOGE-USD\",\n",
    "    # \"Etherium\": \"ETH-USD\",\n",
    "    # \"PolkaDot\": \"DOT-USD\", \n",
    "    # \"Stellar\": \"XLM-USD\",\n",
    "    # \"Monero\": \"XMR-USD\", \n",
    "    # \"Neo\": \"NEO-USD\",\n",
    "    # \"Litecoin\": \"LTC-USD\",\n",
    "    # \"Cardano\": \"ADA-USD\",\n",
    "    # \"Eos\": \"EOS-USD\",\n",
    "\n",
    "    # \"US_Stocks\": \"^GSPC\",\n",
    "    \"Global_Stocks\": \"URTH\",\n",
    "    \"Emerging_Markets\": \"EEM\",\n",
    "    # \"US_Bonds\": \"AGG\",\n",
    "    \"Intl_Bonds\": \"BNDX\",\n",
    "    \"Broad_Commodities\": \"GSG\",\n",
    "    \"Gold\": \"GLD\",\n",
    "    # \"US_RealEstate\": \"VNQ\",\n",
    "    \"Global_RealEstate\": \"VNQI\",\n",
    "    \"Cash\": \"^IRX\",\n",
    "    \"Volatility_Index\": \"^VIX\",\n",
    "    # \"CVIX\": \"CVOL-USD\",\n",
    "    # \"Inflation_Protected\": \"TIP\",\n",
    "    # \"Private_Equity\": \"PSP\",\n",
    "    # \"DollarIndex\": \"^DXY\",   # US Dollar Index\n",
    "\n",
    "    # \"Apple\": \"AAPL\",\n",
    "    # \"Microsoft\": \"MSFT\",\n",
    "    # \"Amazon\": \"AMZN\",\n",
    "    # \"Tesla\": \"TSLA\",\n",
    "    # \"Google\": \"GOOGL\",\n",
    "    # \"Meta\": \"META\",\n",
    "    # \"NVIDIA\": \"NVDA\",\n",
    "    # \"ExxonMobil\": \"XOM\",\n",
    "    # \"JPMorgan\": \"JPM\",\n",
    "    # \"Johnson_Johnson\": \"JNJ\",\n",
    "    # \"Procter_Gamble\": \"PG\",\n",
    "    # \"Pfizer\": \"PFE\",\n",
    "    # \"CocaCola\": \"KO\",\n",
    "    # \"PepsiCo\": \"PEP\",\n",
    "    # \"Intel\": \"INTC\",\n",
    "    # \"Cisco\": \"CSCO\",\n",
    "    # \"Chevron\": \"CVX\",\n",
    "    # \"Goldman_Sachs\": \"GS\",\n",
    "    # \"Boeing\": \"BA\",\n",
    "    # \"Disney\": \"DIS\",\n",
    "    # \"US_Stocks\": \"^GSPC\",\n",
    "    # \"Global_Stocks\": \"URTH\",\n",
    "    # \"Emerging_Markets\": \"EEM\",\n",
    "    # # \"US_Bonds\": \"AGG\",\n",
    "    # # \"Intl_Bonds\": \"BNDX\",\n",
    "    # \"Broad_Commodities\": \"GSG\",\n",
    "    # \"Gold\": \"GLD\",\n",
    "    # \"US_RealEstate\": \"VNQ\",\n",
    "    # \"Global_RealEstate\": \"VNQI\",\n",
    "    # # \"Cash\": \"^IRX\",\n",
    "    # \"Volatility_Index\": \"^VIX\",\n",
    "    # \"Inflation_Protected\": \"TIP\",\n",
    "    # \"Private_Equity\": \"PSP\"\n",
    "}\n",
    "\n",
    "start_date = \"2000-01-01\"  # Try a more recent start date if older data isn't available\n",
    "end_date = \"2025-02-08\"\n",
    "\n",
    "data = {}\n",
    "for name, ticker in tickers.items():\n",
    "    print(f\"Downloading data for {name} ({ticker})...\")\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "    print(df.head())\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No data for {name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # If multi-indexed columns, flatten them\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Attempt to drop levels if they are consistent. For example, \n",
    "        # if you see columns like (Price, ^GSPC, Adj Close), dropping two levels:\n",
    "        df.columns = df.columns.droplevel([1])\n",
    "\n",
    "    print(f\"Columns after flattening for {name}:\", df.columns)\n",
    "\n",
    "    if 'Adj Close' in df.columns:\n",
    "        data[name] = df['Adj Close']\n",
    "    else:\n",
    "        print(f\"'Adj Close' not found in {name}. Available columns: {df.columns}\")\n",
    "\n",
    "if data:\n",
    "    combined_data = pd.DataFrame(data)\n",
    "    combined_data.index.name = \"Date\"\n",
    "    combined_data = combined_data.dropna()\n",
    "    combined_data.to_csv(\"Diversified_Portfolio_Data_Complete_Macro.csv\")\n",
    "    print(\"Data saved to 'Diversified_Portfolio_Data_Complete.csv'\")\n",
    "    print(combined_data)\n",
    "else:\n",
    "    print(\"No valid data collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from ctypes import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TF_EQUIV = {\n",
    "    \"1m\": \"1Min\",\n",
    "    \"5m\": \"5Min\",\n",
    "    \"15m\": \"15Min\",\n",
    "    \"30m\": \"30Min\",\n",
    "    \"1h\": \"1H\",\n",
    "    \"4h\": \"4H\",\n",
    "    \"12h\": \"12H\",\n",
    "    \"1d\": \"D\",\n",
    "}\n",
    "\n",
    "STRAT_PARAMS = {\n",
    "    \"obv\": {\n",
    "        \"ma_period\": {\"name\": \"MA Period\", \"type\": int, \"min\": 2, \"max\": 200},\n",
    "    },\n",
    "    \"ichimoku\": {\n",
    "        \"kijun\": {\"name\": \"Kijun Period\", \"type\": int, \"min\": 25, \"max\": 90},\n",
    "        \"tenkan\": {\"name\": \"Tenkan Period\", \"type\": int, \"min\": 10, \"max\": 50},\n",
    "    },\n",
    "    \"sup_res\": {\n",
    "        \"min_points\": {\"name\": \"Min. Points\", \"type\": int, \"min\": 2, \"max\": 20},\n",
    "        \"min_diff_points\": {\n",
    "            \"name\": \"Min. Difference between Points\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1,\n",
    "            \"max\": 100,\n",
    "        },\n",
    "        \"rounding_nb\": {\n",
    "            \"name\": \"Rounding Number\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.001,\n",
    "            \"max\": 0.05,\n",
    "            \"decimals\": 3,\n",
    "        },\n",
    "        \"ratio\": {\n",
    "            \"name\": \"WIN/LOSS Ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.5,\n",
    "            \"max\": 10,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"risk\": {\n",
    "            \"name\": \"Risk per trade\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.02,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"leverage\": {\"name\": \"Leverage\", \"type\": int, \"min\": 1, \"max\": 50},\n",
    "        \"cash\": {\n",
    "            \"name\": \"Initial balance (USD)\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1000,\n",
    "            \"max\": 1000,\n",
    "        },\n",
    "        \"stop_short\": {\n",
    "            \"name\": \"Stop short safety\",\n",
    "            \"type\": float,\n",
    "            \"min\": 1.001,\n",
    "            \"max\": 1.01,\n",
    "            \"decimals\": 4,\n",
    "        },\n",
    "        \"stop_long\": {\n",
    "            \"name\": \"Stop long safety\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.99,\n",
    "            \"max\": 0.999,\n",
    "            \"decimals\": 4,\n",
    "        },\n",
    "    },\n",
    "    \"fractals\": {\n",
    "        \"ema_fast\": {\"name\": \"EMA Fast\", \"type\": int, \"min\": 6, \"max\": 50},\n",
    "        \"ema_middle\": {\"name\": \"EMA Middle\", \"type\": int, \"min\": 40, \"max\": 100},\n",
    "        \"ema_slow\": {\"name\": \"EMA Slow\", \"type\": int, \"min\": 100, \"max\": 400},\n",
    "        \"rsi_length\": {\"name\": \"RSI\", \"type\": int, \"min\": 14, \"max\": 14},\n",
    "        \"stop_ratio\": {\n",
    "            \"name\": \"WIN/LOSS Ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 1,\n",
    "            \"max\": 3,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"risk\": {\n",
    "            \"name\": \"Risk per trade\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.02,\n",
    "            \"decimals\": 3,\n",
    "        },\n",
    "        \"cash\": {\n",
    "            \"name\": \"Initial balance (USD)\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1000,\n",
    "            \"max\": 1000,\n",
    "        },\n",
    "        \"leverage\": {\"name\": \"Leverage\", \"type\": int, \"min\": 1, \"max\": 50},\n",
    "    },\n",
    "    \"fractals2\": {\n",
    "        \"ema_fast\": {\"name\": \"EMA Fast\", \"type\": int, \"min\": 1, \"max\": 50},\n",
    "        \"ema_middle\": {\"name\": \"EMA Middle\", \"type\": int, \"min\": 40, \"max\": 100},\n",
    "        \"ema_slow\": {\"name\": \"EMA Slow\", \"type\": int, \"min\": 100, \"max\": 400},\n",
    "        \"rsi_length\": {\"name\": \"RSI\", \"type\": int, \"min\": 1, \"max\": 100},\n",
    "        \"stop_ratio\": {\n",
    "            \"name\": \"WIN/LOSS Ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.5,\n",
    "            \"max\": 10,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"risk\": {\n",
    "            \"name\": \"Risk per trade\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.02,\n",
    "            \"decimals\": 3,\n",
    "        },\n",
    "        \"cash\": {\n",
    "            \"name\": \"Initial balance (USD)\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1000,\n",
    "            \"max\": 1000,\n",
    "        },\n",
    "        \"leverage\": {\"name\": \"Leverage\", \"type\": int, \"min\": 1, \"max\": 125},\n",
    "        \"breakeven\": {\n",
    "            \"name\": \"Break Even\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 1,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "    },\n",
    "    \"fractal_simple\": {\n",
    "        \"ema_fast\": {\"name\": \"EMA Fast\", \"type\": int, \"min\": 1, \"max\": 50},\n",
    "        \"ema_middle\": {\"name\": \"EMA Middle\", \"type\": int, \"min\": 40, \"max\": 100},\n",
    "        \"ema_slow\": {\"name\": \"EMA Slow\", \"type\": int, \"min\": 100, \"max\": 400},\n",
    "        \"rsi_period\": {\"name\": \"RSI\", \"type\": int, \"min\": 14, \"max\": 14},\n",
    "        \"bk_ratio\": {\n",
    "            \"name\": \"Break Even\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 0.9,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"ratio\": {\n",
    "            \"name\": \"WIN/LOSS Ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.5,\n",
    "            \"max\": 3,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"risk\": {\n",
    "            \"name\": \"Risk per trade\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.02,\n",
    "            \"decimals\": 3,\n",
    "        },\n",
    "        \"cash\": {\n",
    "            \"name\": \"Initial balance (USD)\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1000,\n",
    "            \"max\": 1000,\n",
    "        },\n",
    "        \"leverage\": {\"name\": \"Leverage\", \"type\": int, \"min\": 1, \"max\": 125},\n",
    "    },\n",
    "    \"sma\": {\n",
    "        \"slow_ma\": {\"name\": \"Slow MA Period\", \"type\": int, \"min\": 2, \"max\": 200},\n",
    "        \"fast_ma\": {\"name\": \"Fast MA Period\", \"type\": int, \"min\": 2, \"max\": 200},\n",
    "    },\n",
    "    \"psar\": {\n",
    "        \"initial_acc\": {\n",
    "            \"name\": \"Initial Acceleration\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.2,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"acc_increment\": {\n",
    "            \"name\": \"Acceleration Increment\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.2,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"max_acc\": {\n",
    "            \"name\": \"Max. Acceleration\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.05,\n",
    "            \"max\": 2,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "    },\n",
    "    \"fractal_test\": {\n",
    "        \"ema_fast\": {\"name\": \"EMA Fast\", \"type\": int, \"min\": 2, \"max\": 8},\n",
    "        \"ema_middle\": {\"name\": \"EMA Middle\", \"type\": int, \"min\": 90, \"max\": 110},\n",
    "        \"ema_slow\": {\"name\": \"EMA Slow\", \"type\": int, \"min\": 100, \"max\": 200},\n",
    "        \"rsi_period\": {\"name\": \"RSI Period\", \"type\": int, \"min\": 50, \"max\": 70},\n",
    "        \"rsi_long_value\": {\n",
    "            \"name\": \"RSI Long Value Condition\",\n",
    "            \"type\": int,\n",
    "            \"min\": 70,\n",
    "            \"max\": 90,\n",
    "        },\n",
    "        \"rsi_short_value\": {\n",
    "            \"name\": \"RSI Short Value Condition\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1,\n",
    "            \"max\": 10,\n",
    "        },\n",
    "        \"ratio\": {\n",
    "            \"name\": \"WIN/LOSS Ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 7,\n",
    "            \"max\": 10,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"risk\": {\n",
    "            \"name\": \"Risk per trade\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.02,\n",
    "            \"max\": 0.02,\n",
    "            \"decimals\": 3,\n",
    "        },\n",
    "        \"cash\": {\n",
    "            \"name\": \"Initial balance (USD)\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1000,\n",
    "            \"max\": 1000,\n",
    "        },\n",
    "        \"leverage\": {\"name\": \"Leverage\", \"type\": int, \"min\": 1, \"max\": 15},\n",
    "        \"bk_ratio\": {\n",
    "            \"name\": \"Breakeven ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.9,\n",
    "            \"max\": 1,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"dist_long\": {\n",
    "            \"name\": \"Stop loss long distance\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.999,\n",
    "            \"max\": 1,\n",
    "            \"decimals\": 4,\n",
    "        },\n",
    "        \"dist_short\": {\n",
    "            \"name\": \"Stop loss short distance\",\n",
    "            \"type\": float,\n",
    "            \"min\": 1,\n",
    "            \"max\": 1.001,\n",
    "            \"decimals\": 4,\n",
    "        },\n",
    "        \"nb_lows\": {\"name\": \"Down Trends precisions\", \"type\": int, \"min\": 1, \"max\": 5},\n",
    "        \"nb_highs\": {\"name\": \"Up Trends precisions\", \"type\": int, \"min\": 1, \"max\": 5},\n",
    "        \"macd_line_fast\": {\"name\": \"MACD Fast line\", \"type\": int, \"min\": 12, \"max\": 12},\n",
    "        \"macd_line_slow\": {\"name\": \"MACD Slow line\", \"type\": int, \"min\": 26, \"max\": 26},\n",
    "        \"macd_signal\": {\"name\": \"MACD Signal\", \"type\": int, \"min\": 9, \"max\": 9},\n",
    "        \"macd_long_ratio\": {\n",
    "            \"name\": \"MACD Long ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 1,\n",
    "            \"max\": 3,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"macd_short_ratio\": {\n",
    "            \"name\": \"MACD Short ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.01,\n",
    "            \"max\": 0.1,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "    },\n",
    "    \"sup_res_cpp\": {\n",
    "        \"min_points\": {\"name\": \"Min. Points\", \"type\": int, \"min\": 2, \"max\": 20},\n",
    "        \"min_diff_points\": {\n",
    "            \"name\": \"Min. Difference between Points\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1,\n",
    "            \"max\": 100,\n",
    "        },\n",
    "        \"rounding_nb\": {\n",
    "            \"name\": \"Rounding Number\",\n",
    "            \"type\": float,\n",
    "            \"min\": 10,\n",
    "            \"max\": 500,\n",
    "            \"decimals\": 1,\n",
    "        },\n",
    "        \"ratio\": {\n",
    "            \"name\": \"WIN/LOSS Ratio\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.5,\n",
    "            \"max\": 5,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"risk\": {\n",
    "            \"name\": \"Risk per trade\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.005,\n",
    "            \"max\": 0.02,\n",
    "            \"decimals\": 2,\n",
    "        },\n",
    "        \"leverage\": {\"name\": \"Leverage\", \"type\": int, \"min\": 1, \"max\": 50},\n",
    "        \"cash\": {\n",
    "            \"name\": \"Initial balance (USD)\",\n",
    "            \"type\": int,\n",
    "            \"min\": 1000,\n",
    "            \"max\": 1000,\n",
    "        },\n",
    "        \"stop_short\": {\n",
    "            \"name\": \"Stop short safety\",\n",
    "            \"type\": float,\n",
    "            \"min\": 1.0001,\n",
    "            \"max\": 1.01,\n",
    "            \"decimals\": 4,\n",
    "        },\n",
    "        \"stop_long\": {\n",
    "            \"name\": \"Stop long safety\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.99,\n",
    "            \"max\": 0.9999,\n",
    "            \"decimals\": 4,\n",
    "        },\n",
    "    },\n",
    "    \"bollinger\": {\n",
    "        \"window_size\": {\n",
    "            \"name\": \"window size\",\n",
    "            \"type\": int,\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 1000000,\n",
    "        },\n",
    "        \"num_std\": {\n",
    "            \"name\": \"deviation number\",\n",
    "            \"type\": float,\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 100000,\n",
    "        },\n",
    "    },\n",
    "    \"single_index\": {\"test\": {\"name\": \"test\", \"type\": int}},\n",
    "    \"arima\": {\"start_date\": {\"name\": \"start_date\", \"type\": str}},\n",
    "    # \"drl\": {\n",
    "    #     \"batch_size\": {\"name\": \"batch_size\", \"type\": int}\n",
    "    # }\n",
    "}\n",
    "\n",
    "\n",
    "def ms_to_dt(ms: int) -> datetime.datetime:\n",
    "    return datetime.datetime.utcfromtimestamp(ms / 1000)\n",
    "\n",
    "\n",
    "# def resample_timeframe(data: pd.DataFrame, tf: str) -> pd.DataFrame:\n",
    "#     return data.resample(TF_EQUIV[tf]).agg(\n",
    "#         {\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\", \"volume\": \"sum\"}\n",
    "#     )\n",
    "\n",
    "\n",
    "def resample_timeframe(data: pd.DataFrame, tf: str) -> pd.DataFrame:\n",
    "    # Identify all columns that represent an instrument.\n",
    "    # For each instrument, we expect columns like {prefix}_open, {prefix}_high, etc.\n",
    "    # The prefix might be empty for the main dataset columns.\n",
    "\n",
    "    # Possible base OHLCV columns\n",
    "    base_cols = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "\n",
    "    # Find all instruments by extracting the prefix from columns that end with '_open'\n",
    "    # or if the base columns exist without any prefix.\n",
    "    instruments = set()\n",
    "    for col in data.columns:\n",
    "        if col.endswith(\"_open\"):\n",
    "            prefix = col[:-5]  # remove '_open'\n",
    "            instruments.add(prefix)\n",
    "\n",
    "    # Check if we have a base instrument (no prefix)\n",
    "    if base_cols.issubset(data.columns):\n",
    "        instruments.add(\"\")  # represents the main (no-prefix) instrument\n",
    "\n",
    "    # Build a dynamic aggregation dictionary\n",
    "    agg_dict = {}\n",
    "    for inst in instruments:\n",
    "        # If inst is empty, columns are just 'open', 'high', 'low', 'close', 'volume'\n",
    "        # Otherwise they are 'inst_open', 'inst_high', etc.\n",
    "        prefix = f\"{inst}_\" if inst else \"\"\n",
    "        agg_dict[f\"{prefix}open\"] = \"first\"\n",
    "        agg_dict[f\"{prefix}high\"] = \"max\"\n",
    "        agg_dict[f\"{prefix}low\"] = \"min\"\n",
    "        agg_dict[f\"{prefix}close\"] = \"last\"\n",
    "        agg_dict[f\"{prefix}volume\"] = \"sum\"\n",
    "\n",
    "    return data.resample(TF_EQUIV[tf]).agg(agg_dict)\n",
    "\n",
    "\n",
    "def get_library():\n",
    "    lib = CDLL(\"backtestingCpp/build/libbacktestingCpp.dll\", winmode=0)\n",
    "\n",
    "    # SMA\n",
    "    lib.Sma_new.restype = c_void_p\n",
    "    lib.Sma_new.argtypes = [c_char_p, c_char_p, c_char_p, c_longlong, c_longlong]\n",
    "    lib.Sma_execute_backtest.restype = c_void_p\n",
    "    lib.Sma_execute_backtest.argtypes = [c_void_p, c_int, c_int]\n",
    "\n",
    "    lib.Sma_get_pnl.restype = c_double\n",
    "    lib.Sma_get_pnl.argtypes = [c_void_p]\n",
    "    lib.Sma_get_max_dd.restype = c_double\n",
    "    lib.Sma_get_max_dd.argtypes = [c_void_p]\n",
    "\n",
    "    # PSAR\n",
    "    lib.Psar_new.restype = c_void_p\n",
    "    lib.Psar_new.argtypes = [c_char_p, c_char_p, c_char_p, c_longlong, c_longlong]\n",
    "    lib.Psar_execute_backtest.restype = c_void_p\n",
    "    lib.Psar_execute_backtest.argtypes = [c_void_p, c_double, c_double, c_double]\n",
    "\n",
    "    lib.Psar_get_pnl.restype = c_double\n",
    "    lib.Psar_get_pnl.argtypes = [c_void_p]\n",
    "    lib.Psar_get_max_dd.restype = c_double\n",
    "    lib.Psar_get_max_dd.argtypes = [c_void_p]\n",
    "\n",
    "    # Fractal_test\n",
    "    lib.fractal_test_new.restype = c_void_p\n",
    "    lib.fractal_test_new.argtypes = [\n",
    "        c_char_p,\n",
    "        c_char_p,\n",
    "        c_char_p,\n",
    "        c_longlong,\n",
    "        c_longlong,\n",
    "    ]\n",
    "    lib.fractal_test_execute_backtest.restype = c_void_p\n",
    "    lib.fractal_test_execute_backtest.argtypes = [\n",
    "        c_void_p,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_int,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_double,\n",
    "        c_double,\n",
    "    ]\n",
    "\n",
    "    lib.fractal_test_get_pnl.restype = c_double\n",
    "    lib.fractal_test_get_pnl.argtypes = [c_void_p]\n",
    "    lib.fractal_test_get_max_dd.restype = c_double\n",
    "    lib.fractal_test_get_max_dd.argtypes = [c_void_p]\n",
    "\n",
    "    # Support Resistance CPP\n",
    "    lib.sup_res_cpp_new.restype = c_void_p\n",
    "    lib.sup_res_cpp_new.argtypes = [\n",
    "        c_char_p,\n",
    "        c_char_p,\n",
    "        c_char_p,\n",
    "        c_longlong,\n",
    "        c_longlong,\n",
    "    ]\n",
    "    lib.sup_res_cpp_execute_backtest.restype = c_void_p\n",
    "    lib.sup_res_cpp_execute_backtest.argtypes = [\n",
    "        c_void_p,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_double,\n",
    "        c_double,\n",
    "    ]\n",
    "\n",
    "    lib.sup_res_cpp_get_pnl.restype = c_double\n",
    "    lib.sup_res_cpp_get_pnl.argtypes = [c_void_p]\n",
    "    lib.sup_res_cpp_get_daily_pnl.restype = c_double\n",
    "    lib.sup_res_cpp_get_daily_pnl.argtypes = [c_void_p]\n",
    "    lib.sup_res_cpp_get_daily_trades.restype = c_int\n",
    "    lib.sup_res_cpp_get_daily_trades.argtypes = [c_void_p]\n",
    "    lib.sup_res_cpp_get_zero_per_day.restype = c_int\n",
    "    lib.sup_res_cpp_get_zero_per_day.argtypes = [c_void_p]\n",
    "\n",
    "    # Fractal_simple\n",
    "    lib.fractal_simple_new.restype = c_void_p\n",
    "    lib.fractal_simple_new.argtypes = [\n",
    "        c_char_p,\n",
    "        c_char_p,\n",
    "        c_char_p,\n",
    "        c_longlong,\n",
    "        c_longlong,\n",
    "    ]\n",
    "    lib.fractal_simple_execute_backtest.restype = c_void_p\n",
    "    lib.fractal_simple_execute_backtest.argtypes = [\n",
    "        c_void_p,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_int,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_double,\n",
    "        c_int,\n",
    "        c_int,\n",
    "    ]\n",
    "\n",
    "    lib.fractal_simple_get_pnl.restype = c_double\n",
    "    lib.fractal_simple_get_pnl.argtypes = [c_void_p]\n",
    "    lib.fractal_simple_get_max_dd.restype = c_double\n",
    "    lib.fractal_simple_get_max_dd.argtypes = [c_void_p]\n",
    "\n",
    "    return lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig()\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "log.info(\"%s logger started.\", __name__)\n",
    "\n",
    "TF_EQUIV = {\n",
    "    \"1m\": \"1Min\",\n",
    "    \"5m\": \"5Min\",\n",
    "    \"15m\": \"15Min\",\n",
    "    \"30m\": \"30Min\",\n",
    "    \"1h\": \"1H\",\n",
    "    \"4h\": \"4H\",\n",
    "    \"12h\": \"12H\",\n",
    "    \"1d\": \"D\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class Hdf5client:\n",
    "    def __init__(self, exchange: str):\n",
    "        self.hf = h5py.File(f\"data/{exchange}.h5\", \"a\")\n",
    "        self.hf.flush()\n",
    "\n",
    "    def create_dataset(self, symbol: str):\n",
    "        if symbol not in self.hf.keys():\n",
    "            self.hf.create_dataset(symbol, (0, 6), maxshape=(None, 6), dtype=\"float64\")\n",
    "            self.hf.flush()\n",
    "\n",
    "    def write_data(self, symbol: str, data: List[Tuple]):\n",
    "\n",
    "        min_ts, max_ts = self.get_first_last_timestamp(symbol)\n",
    "\n",
    "        if min_ts is None:\n",
    "            min_ts = float(\"inf\")\n",
    "            max_ts = 0\n",
    "\n",
    "        filtered_data = []\n",
    "\n",
    "        for d in data:\n",
    "            if d[0] < min_ts:\n",
    "                filtered_data.append(d)\n",
    "            elif d[0] > max_ts:\n",
    "                filtered_data.append(d)\n",
    "\n",
    "        if len(filtered_data) == 0:\n",
    "            logger.warning(\"%s: No data to insert\", symbol)\n",
    "\n",
    "        data_array = np.array(data)\n",
    "\n",
    "        self.hf[symbol].resize(self.hf[symbol].shape[0] + data_array.shape[0], axis=0)\n",
    "        self.hf[symbol][-data_array.shape[0] :] = data_array\n",
    "\n",
    "        self.hf.flush()\n",
    "\n",
    "    def get_data(\n",
    "        self, symbol: str, from_time: int, to_time: int\n",
    "    ) -> Union[None, pd.DataFrame]:\n",
    "\n",
    "        start_query = time.time()\n",
    "\n",
    "        existing_data = self.hf[symbol][:]\n",
    "\n",
    "        if len(existing_data) == 0:\n",
    "            return None\n",
    "\n",
    "        data = sorted(existing_data, key=lambda x: x[0])\n",
    "        data = np.array(data)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "        )\n",
    "        df = df[(df[\"timestamp\"] >= from_time) & (df[\"timestamp\"] <= to_time)]\n",
    "\n",
    "        df[\"timestamp\"] = pd.to_datetime(\n",
    "            df[\"timestamp\"].values.astype(np.int64), unit=\"ms\"\n",
    "        )\n",
    "        df.set_index(\"timestamp\", drop=True, inplace=True)\n",
    "\n",
    "        query_time = round((time.time() - start_query), 2)\n",
    "\n",
    "        logger.info(\n",
    "            \"Retrieved %s %s data from database in %s seconds\",\n",
    "            len(df.index),\n",
    "            symbol,\n",
    "            query_time,\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_data2(\n",
    "        self, symbol: str, from_time: int, to_time: int, tf: str\n",
    "    ) -> Union[None, pd.DataFrame]:\n",
    "\n",
    "        start_query = time.time()\n",
    "\n",
    "        if tf == \"1m\":\n",
    "            params = 60\n",
    "        elif tf == \"5m\":\n",
    "            params = 5 * 60\n",
    "        elif tf == \"15m\":\n",
    "            params = 15 * 60\n",
    "        elif tf == \"30m\":\n",
    "            params = 30 * 60\n",
    "        elif tf == \"1h\":\n",
    "            params = 60 * 60\n",
    "        elif tf == \"4h\":\n",
    "            params = 4 * 60 * 60\n",
    "        elif tf == \"12h\":\n",
    "            params = 12 * 60 * 60\n",
    "        elif tf == \"1d\":\n",
    "            params = 24 * 60 * 60\n",
    "\n",
    "        existing_data = self.hf[symbol][:]\n",
    "\n",
    "        if len(existing_data) == 0:\n",
    "            return None\n",
    "\n",
    "        data = sorted(existing_data, key=lambda x: x[0])\n",
    "        data = np.array(data)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "        )\n",
    "        df = df[\n",
    "            (df[\"timestamp\"] >= (from_time - params * 1000))\n",
    "            & (df[\"timestamp\"] <= (to_time - params * 1000))\n",
    "        ]\n",
    "\n",
    "        df[\"timestamp\"] = pd.to_datetime(\n",
    "            df[\"timestamp\"].values.astype(np.int64), unit=\"ms\"\n",
    "        )\n",
    "        df.set_index(\"timestamp\", drop=True, inplace=True)\n",
    "\n",
    "        df = resample_timeframe(df, tf)\n",
    "\n",
    "        query_time = round((time.time() - start_query), 2)\n",
    "\n",
    "        logger.info(\n",
    "            \"Retrieved %s %s data from database in %s seconds\",\n",
    "            len(df.index),\n",
    "            symbol,\n",
    "            query_time,\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_first_last_timestamp(\n",
    "        self, symbol: str\n",
    "    ) -> Union[Tuple[None, None], Tuple[float, float]]:\n",
    "\n",
    "        existing_data = self.hf[symbol][:]\n",
    "\n",
    "        if len(existing_data) == 0:\n",
    "            return None, None\n",
    "\n",
    "        first_ts = min(existing_data, key=lambda x: x[0])[0]\n",
    "        last_ts = max(existing_data, key=lambda x: x[0])[0]\n",
    "\n",
    "        return first_ts, last_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import logging\n",
    "\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def collect_all(client: typing.Union[BinanceClient], exchange: str, symbol: str):\n",
    "\n",
    "    h5_db = Hdf5client(exchange)\n",
    "    h5_db.create_dataset(symbol)\n",
    "\n",
    "    oldest_ts, most_recent_ts = h5_db.get_first_last_timestamp(symbol)\n",
    "\n",
    "    # Initial Request\n",
    "\n",
    "    if oldest_ts is None:\n",
    "        data = client.get_historical_data(\n",
    "            symbol, end_time=int(time.time() * 1000) - 60000\n",
    "        )\n",
    "\n",
    "        if len(data) == 0:\n",
    "            logger.warning(\"%s %s: no initial data found\", exchange, symbol)\n",
    "            return\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"%s %s: Collected %s initial data from %s to %s\",\n",
    "                exchange,\n",
    "                symbol,\n",
    "                len(data),\n",
    "                ms_to_dt(data[0][0]),\n",
    "                ms_to_dt(data[-1][0]),\n",
    "            )\n",
    "\n",
    "        oldest_ts = data[0][0]\n",
    "        most_recent_ts = data[-1][0]\n",
    "\n",
    "        h5_db.write_data(symbol, data)\n",
    "\n",
    "    data_to_insert = []\n",
    "\n",
    "    # Most recent data\n",
    "\n",
    "    while True:\n",
    "\n",
    "        data = client.get_historical_data(\n",
    "            symbol, start_time=int(most_recent_ts + 60000)\n",
    "        )\n",
    "\n",
    "        if data is None:\n",
    "            time.sleep(4)  # Pause in case an error occurs during the request\n",
    "            continue\n",
    "\n",
    "        if len(data) < 2:\n",
    "            break\n",
    "\n",
    "        data = data[:-1]\n",
    "\n",
    "        data_to_insert = data_to_insert + data\n",
    "\n",
    "        if len(data_to_insert) > 10000:\n",
    "            h5_db.write_data(symbol, data_to_insert)\n",
    "            data_to_insert.clear()\n",
    "\n",
    "        if data[-1][0] > most_recent_ts:\n",
    "            most_recent_ts = data[-1][0]\n",
    "\n",
    "        logger.info(\n",
    "            \"%s %s: Collected %s recent data from %s to %s\",\n",
    "            exchange,\n",
    "            symbol,\n",
    "            len(data),\n",
    "            ms_to_dt(data[0][0]),\n",
    "            ms_to_dt(data[-1][0]),\n",
    "        )\n",
    "\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    h5_db.write_data(symbol, data_to_insert)\n",
    "    data_to_insert.clear()\n",
    "\n",
    "    # Older data\n",
    "\n",
    "    while True:\n",
    "\n",
    "        data = client.get_historical_data(symbol, end_time=int(oldest_ts - 60000))\n",
    "\n",
    "        if data is None:\n",
    "            time.sleep(4)  # Pause in case an error occurs during the request\n",
    "            continue\n",
    "\n",
    "        if len(data) == 0:\n",
    "            logger.info(\n",
    "                \"%s %s: Stopped older data collection because no data was found before %s\",\n",
    "                exchange,\n",
    "                symbol,\n",
    "                ms_to_dt(oldest_ts),\n",
    "            )\n",
    "            break\n",
    "\n",
    "        data_to_insert = data_to_insert + data\n",
    "\n",
    "        if len(data_to_insert) > 10000:\n",
    "            h5_db.write_data(symbol, data_to_insert)\n",
    "            data_to_insert.clear()\n",
    "\n",
    "        if data[0][0] < oldest_ts:\n",
    "            oldest_ts = data[0][0]\n",
    "\n",
    "        logger.info(\n",
    "            \"%s %s: Collected %s older data from %s to %s\",\n",
    "            exchange,\n",
    "            symbol,\n",
    "            len(data),\n",
    "            ms_to_dt(data[0][0]),\n",
    "            ms_to_dt(data[-1][0]),\n",
    "        )\n",
    "\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    h5_db.write_data(symbol, data_to_insert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class BinanceClient:\n",
    "    def __init__(self, futures=False):\n",
    "\n",
    "        self.futures = futures\n",
    "\n",
    "        if self.futures:\n",
    "            self._base_url = \"https://fapi.binance.com\"\n",
    "        else:\n",
    "            self._base_url = \"https://api.binance.com\"\n",
    "\n",
    "        self.symbols = self._get_symbols()\n",
    "\n",
    "    def _make_request(self, endpoint: str, query_parameters: Dict):\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self._base_url + endpoint, params=query_parameters)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Connection error while making request to %s: %s\", endpoint, e)\n",
    "            return None\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            logger.error(\n",
    "                \"Error while making request to %s: %s (status code == %s)\",\n",
    "                endpoint,\n",
    "                response.json(),\n",
    "                response.status_code,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    def _get_symbols(self) -> List[str]:\n",
    "\n",
    "        params = dict()\n",
    "\n",
    "        endpoint = \"/fapi/v1/exchangeInfo\" if self.futures else \"/api/v3/exchangeInfo\"\n",
    "        data = self._make_request(endpoint, params)\n",
    "\n",
    "        symbols = [x[\"symbol\"] for x in data[\"symbols\"]]\n",
    "\n",
    "        print(symbols)\n",
    "\n",
    "        return symbols\n",
    "\n",
    "    def get_historical_data(\n",
    "        self,\n",
    "        symbol: str,\n",
    "        start_time: Optional[int] = None,\n",
    "        end_time: Optional[int] = None,\n",
    "    ):\n",
    "\n",
    "        params = dict()\n",
    "\n",
    "        params[\"symbol\"] = symbol\n",
    "        params[\"interval\"] = \"1m\"\n",
    "        params[\"limit\"] = 1500\n",
    "\n",
    "        if start_time is not None:\n",
    "            params[\"startTime\"] = start_time\n",
    "        if end_time is not None:\n",
    "            params[\"endTime\"] = end_time\n",
    "\n",
    "        endpoint = \"/fapi/v1/klines\" if self.futures else \"/api/v3/exchangeInfo\"\n",
    "        raw_candles = self._make_request(endpoint, params)\n",
    "\n",
    "        candles = []\n",
    "\n",
    "        if raw_candles is not None:\n",
    "            for c in raw_candles:\n",
    "                candles.append(\n",
    "                    (\n",
    "                        float(c[0]),\n",
    "                        float(c[1]),\n",
    "                        float(c[2]),\n",
    "                        float(c[3]),\n",
    "                        float(c[4]),\n",
    "                        float(c[5]),\n",
    "                    )\n",
    "                )\n",
    "            return candles\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1m':                      btcusdt_open  btcusdt_high  btcusdt_low  btcusdt_close  \\\n",
      "timestamp                                                                     \n",
      "2018-06-11 11:30:00       6761.28       6772.99      6761.28        6765.00   \n",
      "2018-06-11 11:31:00       6765.00       6765.85      6758.57        6760.00   \n",
      "2018-06-11 11:32:00       6760.00       6768.27      6760.00        6765.77   \n",
      "2018-06-11 11:33:00       6765.77       6765.77      6762.06        6765.75   \n",
      "2018-06-11 11:34:00       6765.76       6765.77      6764.69        6765.76   \n",
      "...                           ...           ...          ...            ...   \n",
      "2025-02-07 22:56:00      95821.50      95826.00     95821.40       95825.30   \n",
      "2025-02-07 22:57:00      95825.30      95891.20     95825.20       95891.20   \n",
      "2025-02-07 22:58:00      95892.00      96058.00     95892.00       96008.60   \n",
      "2025-02-07 22:59:00      96008.10      96024.00     95975.20       96014.30   \n",
      "2025-02-07 23:00:00      96014.20      96014.30     95861.10       95877.90   \n",
      "\n",
      "                     btcusdt_volume  ethusdt_open  ethusdt_high  ethusdt_low  \\\n",
      "timestamp                                                                      \n",
      "2018-06-11 11:30:00        3.433960        531.93        532.45       531.39   \n",
      "2018-06-11 11:31:00       11.539627        531.65        531.80       531.39   \n",
      "2018-06-11 11:32:00       10.598203        531.65        531.77       531.06   \n",
      "2018-06-11 11:33:00        2.724804        531.62        532.12       531.48   \n",
      "2018-06-11 11:34:00        6.028348        531.74        532.15       531.50   \n",
      "...                             ...           ...           ...          ...   \n",
      "2025-02-07 22:56:00       22.834000       2577.43       2578.67      2576.99   \n",
      "2025-02-07 22:57:00       67.510000       2578.67       2581.00      2577.91   \n",
      "2025-02-07 22:58:00      123.523000       2580.57       2586.00      2580.57   \n",
      "2025-02-07 22:59:00       99.044000       2584.50       2584.76      2582.23   \n",
      "2025-02-07 23:00:00      121.443000       2582.24       2582.24      2576.81   \n",
      "\n",
      "                     ethusdt_close  ethusdt_volume  ...  ltcusdt_open  \\\n",
      "timestamp                                           ...                 \n",
      "2018-06-11 11:30:00         531.85        93.16106  ...        106.76   \n",
      "2018-06-11 11:31:00         531.65        75.65343  ...        106.78   \n",
      "2018-06-11 11:32:00         531.77        45.77927  ...        106.89   \n",
      "2018-06-11 11:33:00         531.89        26.13004  ...        106.84   \n",
      "2018-06-11 11:34:00         531.89        58.14813  ...        106.90   \n",
      "...                            ...             ...  ...           ...   \n",
      "2025-02-07 22:56:00        2578.66      1810.59500  ...        101.34   \n",
      "2025-02-07 22:57:00        2580.56      2222.81500  ...        101.34   \n",
      "2025-02-07 22:58:00        2584.51      3745.46600  ...        101.43   \n",
      "2025-02-07 22:59:00        2582.23      2539.10900  ...        101.63   \n",
      "2025-02-07 23:00:00        2579.73      3367.53400  ...        101.66   \n",
      "\n",
      "                     ltcusdt_high  ltcusdt_low  ltcusdt_close  ltcusdt_volume  \\\n",
      "timestamp                                                                       \n",
      "2018-06-11 11:30:00        106.80       106.72         106.78        15.18664   \n",
      "2018-06-11 11:31:00        106.78       106.78         106.78        10.33512   \n",
      "2018-06-11 11:32:00        106.89       106.84         106.84         0.13100   \n",
      "2018-06-11 11:33:00        106.90       106.84         106.90        21.00000   \n",
      "2018-06-11 11:34:00        106.90       106.90         106.90         5.77448   \n",
      "...                           ...          ...            ...             ...   \n",
      "2025-02-07 22:56:00        101.37       101.26         101.33      1041.07500   \n",
      "2025-02-07 22:57:00        101.43       101.34         101.43       452.25500   \n",
      "2025-02-07 22:58:00        101.69       101.43         101.64      2332.51900   \n",
      "2025-02-07 22:59:00        101.68       101.60         101.67      1190.55100   \n",
      "2025-02-07 23:00:00        101.67       101.39         101.46      2058.32700   \n",
      "\n",
      "                     eosusdt_open  eosusdt_high  eosusdt_low  eosusdt_close  \\\n",
      "timestamp                                                                     \n",
      "2018-06-11 11:30:00       11.3401       11.3466      11.3097        11.3338   \n",
      "2018-06-11 11:31:00       11.3347       11.3356      11.3204        11.3206   \n",
      "2018-06-11 11:32:00       11.3364       11.3438      11.3268        11.3438   \n",
      "2018-06-11 11:33:00       11.3438       11.3600      11.3332        11.3593   \n",
      "2018-06-11 11:34:00       11.3593       11.3641      11.3448        11.3641   \n",
      "...                           ...           ...          ...            ...   \n",
      "2025-02-07 22:56:00        0.5730        0.5740       0.5720         0.5740   \n",
      "2025-02-07 22:57:00        0.5730        0.5740       0.5730         0.5740   \n",
      "2025-02-07 22:58:00        0.5740        0.5770       0.5730         0.5760   \n",
      "2025-02-07 22:59:00        0.5760        0.5760       0.5750         0.5750   \n",
      "2025-02-07 23:00:00        0.5740        0.5750       0.5730         0.5730   \n",
      "\n",
      "                     eosusdt_volume  \n",
      "timestamp                            \n",
      "2018-06-11 11:30:00          943.79  \n",
      "2018-06-11 11:31:00         9782.28  \n",
      "2018-06-11 11:32:00         1229.61  \n",
      "2018-06-11 11:33:00         2784.00  \n",
      "2018-06-11 11:34:00         6153.99  \n",
      "...                             ...  \n",
      "2025-02-07 22:56:00        81046.10  \n",
      "2025-02-07 22:57:00        24860.50  \n",
      "2025-02-07 22:58:00       131078.40  \n",
      "2025-02-07 22:59:00        89000.80  \n",
      "2025-02-07 23:00:00        40708.70  \n",
      "\n",
      "[3500594 rows x 50 columns]}\n"
     ]
    }
   ],
   "source": [
    "# Data retrieving\n",
    "def get_timeframe_data(symbol, from_time, to_time, timeframe):\n",
    "    h5_db = Hdf5client(\"binance\")\n",
    "    data = h5_db.get_data(symbol, from_time, to_time)\n",
    "    if timeframe != \"1m\":\n",
    "        data = resample_timeframe(data, timeframe)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_additional_data(file_path, asset_prefix, timeframe):\n",
    "    \"\"\"\n",
    "    Prepares additional data in the same format as the EURUSD example and resamples it\n",
    "    to match the provided timeframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the CSV file.\n",
    "    asset_prefix : str\n",
    "        The prefix to prepend to column names, e.g. 'eurusd' or 'ustbond'.\n",
    "    timeframe : str\n",
    "        The target timeframe to which the data should be resampled (e.g., '4h', '1h', etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame indexed by timestamp at the specified timeframe and columns renamed\n",
    "        with the asset_prefix.\n",
    "    \"\"\"\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert the timestamp from milliseconds to datetime\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"Local time\"], unit=\"ms\")\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    # Keep only the required columns\n",
    "    df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "\n",
    "    # Rename columns to include the asset prefix\n",
    "    df.columns = [f\"{asset_prefix}_{col.lower()}\" for col in df.columns]\n",
    "\n",
    "    # The original data is in 1m timeframe by default, so resample if needed\n",
    "    if timeframe != \"1m\":\n",
    "        df = resample_timeframe(df, timeframe)\n",
    "\n",
    "    return df    \n",
    "\n",
    "\n",
    "# Get the total system memory\n",
    "# total_memory = psutil.virtual_memory().total\n",
    "\n",
    "# Calculate 50% of total system memory\n",
    "# memory_to_allocate = total_memory * 0.5\n",
    "\n",
    "from_time = \"2015-01-01\"\n",
    "to_time = \"2025-02-08\"\n",
    "symbol = \"BTCUSDT\"\n",
    "\n",
    "# Define timeframes\n",
    "timeframes = [\"1m\"]\n",
    "tf = timeframes[0]\n",
    "\n",
    "# Convert times\n",
    "from_time = int(\n",
    "    datetime.datetime.strptime(from_time, \"%Y-%m-%d\").timestamp() * 1000\n",
    ")\n",
    "to_time = int(datetime.datetime.strptime(to_time, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "\n",
    "\n",
    "data = get_timeframe_data(symbol, from_time, to_time, tf)\n",
    "ethusdt_df = get_timeframe_data(\"ETHUSDT\", from_time, to_time, tf)\n",
    "bnbusdt_df = get_timeframe_data(\"BNBUSDT\", from_time, to_time, tf)\n",
    "xrpusdt_df = get_timeframe_data(\"XRPUSDT\", from_time, to_time, tf)\n",
    "solusdt_df = get_timeframe_data(\"SOLUSDT\", from_time, to_time, tf)\n",
    "adausdt_df = get_timeframe_data(\"ADAUSDT\", from_time, to_time, tf)\n",
    "dogeusdt_df = get_timeframe_data(\"DOGEUSDT\", from_time, to_time, tf)\n",
    "trxusdt_df = get_timeframe_data(\"TRXUSDT\", from_time, to_time, tf)\n",
    "avaxusdt_df = get_timeframe_data(\"AVAXUSDT\", from_time, to_time, tf)\n",
    "# shibusdt_df = get_timeframe_data(\"1000SHIBUSDT\", from_time, to_time, tf)\n",
    "dotusdt_df = get_timeframe_data(\"DOTUSDT\", from_time, to_time, tf)\n",
    "xlmusdt_df = get_timeframe_data(\"XLMUSDT\", from_time, to_time, tf)\n",
    "xmrusdt_df = get_timeframe_data(\"XMRUSDT\", from_time, to_time, tf)\n",
    "neousdt_df = get_timeframe_data(\"NEOUSDT\", from_time, to_time, tf)\n",
    "ltcusdt_df = get_timeframe_data(\"LTCUSDT\", from_time, to_time, tf)\n",
    "# adausdt_df = get_timeframe_data(\"ADAUSDT\", from_time, to_time, tf)\n",
    "eosusdt_df = get_timeframe_data(\"EOSUSDT\", from_time, to_time, tf)\n",
    "\n",
    "# Rename columns to include the asset prefix\n",
    "data.columns = [f\"btcusdt_{col.lower()}\" for col in data.columns]\n",
    "ethusdt_df.columns = [f\"ethusdt_{col.lower()}\" for col in ethusdt_df.columns]\n",
    "bnbusdt_df.columns = [f\"bnbusdt_{col.lower()}\" for col in bnbusdt_df.columns]\n",
    "xrpusdt_df.columns = [f\"xrpusdt_{col.lower()}\" for col in xrpusdt_df.columns]\n",
    "solusdt_df.columns = [f\"solusdt_{col.lower()}\" for col in solusdt_df.columns]\n",
    "adausdt_df.columns = [f\"adausdt_{col.lower()}\" for col in adausdt_df.columns]\n",
    "dogeusdt_df.columns = [f\"dogeusdt_{col.lower()}\" for col in dogeusdt_df.columns]\n",
    "trxusdt_df.columns = [f\"trxusdt_{col.lower()}\" for col in trxusdt_df.columns]\n",
    "avaxusdt_df.columns = [f\"avaxusdt_{col.lower()}\" for col in avaxusdt_df.columns]\n",
    "# shibusdt_df.columns = [f\"shibusdt_{col.lower()}\" for col in shibusdt_df.columns]\n",
    "dotusdt_df.columns = [f\"dotusdt_{col.lower()}\" for col in dotusdt_df.columns]\n",
    "xlmusdt_df.columns = [f\"xlmusdt_{col.lower()}\" for col in xlmusdt_df.columns]\n",
    "xmrusdt_df.columns = [f\"xmrusdt_{col.lower()}\" for col in xmrusdt_df.columns]\n",
    "neousdt_df.columns = [f\"neousdt_{col.lower()}\" for col in neousdt_df.columns]\n",
    "ltcusdt_df.columns = [f\"ltcusdt_{col.lower()}\" for col in ltcusdt_df.columns]\n",
    "# adausdt_df.columns = [f\"adausdt_{col.lower()}\" for col in adausdt_df.columns]\n",
    "eosusdt_df.columns = [f\"eosusdt_{col.lower()}\" for col in eosusdt_df.columns]\n",
    "\n",
    "# print(data)\n",
    "# print(ethusdt_df)\n",
    "# print(bnbusdt_df)\n",
    "# print(xrpusdt_df)\n",
    "# print(solusdt_df)\n",
    "# print(adausdt_df)\n",
    "# print(dogeusdt_df)\n",
    "# print(trxusdt_df)\n",
    "# print(avaxusdt_df)\n",
    "# # print(shibusdt_df)\n",
    "# print(dotusdt_df)\n",
    "# print(xlmusdt_df)\n",
    "# print(xmrusdt_df)\n",
    "# print(neousdt_df)\n",
    "# print(ltcusdt_df)\n",
    "# # print(adausdt_df)\n",
    "# print(eosusdt_df)\n",
    "\n",
    "\n",
    "data_close_df = data[[\"btcusdt_close\"]]\n",
    "ethusdt_close_df = ethusdt_df[[\"ethusdt_close\"]]\n",
    "bnbusdt_close_df = bnbusdt_df[[\"bnbusdt_close\"]]\n",
    "xrpusdt_close_df = xrpusdt_df[[\"xrpusdt_close\"]]\n",
    "solusdt_close_df = solusdt_df[[\"solusdt_close\"]]\n",
    "adausdt_close_df = adausdt_df[[\"adausdt_close\"]]\n",
    "dogeusdt_close_df = dogeusdt_df[[\"dogeusdt_close\"]]\n",
    "trxusdt_close_df = trxusdt_df[[\"trxusdt_close\"]]\n",
    "avaxusdt_close_df = avaxusdt_df[[\"avaxusdt_close\"]]\n",
    "# shibusdt_close_df = shibusdt_df[[\"shibusdt_close\"]]\n",
    "dotusdt_close_df = dotusdt_df[[\"dotusdt_close\"]]\n",
    "xlmusdt_close_df = xlmusdt_df[[\"xlmusdt_close\"]]\n",
    "xmrusdt_close_df = xmrusdt_df[[\"xmrusdt_close\"]]\n",
    "neousdt_close_df = neousdt_df[[\"neousdt_close\"]]\n",
    "ltcusdt_close_df = ltcusdt_df[[\"ltcusdt_close\"]]\n",
    "# adausdt_close_df = adausdt_df[[\"adausdt_close\"]]\n",
    "eosusdt_close_df = eosusdt_df[[\"eosusdt_close\"]]\n",
    "\n",
    "\n",
    "# Additional data preparation and resampling to match main_data timeframe\n",
    "eurusd_df = prepare_additional_data(\n",
    "    \"data/EURUSD/eurusd_cleaned.csv\", \"eurusd\", timeframe=tf\n",
    ")\n",
    "eurusd_close_df = eurusd_df[[\"eurusd_close\"]]\n",
    "gbpusd_df = prepare_additional_data(\n",
    "    \"data/GBPUSD/gbpusd_cleaned.csv\", \"gbpusd\", timeframe=tf\n",
    ")\n",
    "gbpusd_close_df = gbpusd_df[[\"gbpusd_close\"]]\n",
    "xauusd_df = prepare_additional_data(\n",
    "    \"data/Gold/xauusd_cleaned.csv\", \"xauusd\", timeframe=tf\n",
    ")\n",
    "xauusd_close_df = xauusd_df[[\"xauusd_close\"]]\n",
    "xleusd_df = prepare_additional_data(\n",
    "    \"data/XLE_US_USD/xleusd_cleaned.csv\", \"xleusd\", timeframe=tf\n",
    ")\n",
    "xleusd_close_df = xleusd_df[[\"xleusd_close\"]]\n",
    "xlpusd_df = prepare_additional_data(\n",
    "    \"data/XLP_US_USD/xlpusd_cleaned.csv\", \"xlpusd\", timeframe=tf\n",
    ")\n",
    "xlpusd_close_df = xlpusd_df[[\"xlpusd_close\"]]\n",
    "ustbond_df = prepare_additional_data(\n",
    "    \"data/US_T-Bonds/ustbond_cleaned.csv\", \"ustbond\", timeframe=tf\n",
    ")\n",
    "ustbond_close_df = ustbond_df[[\"ustbond_close\"]]\n",
    "sp500_df = prepare_additional_data(\n",
    "    \"data/SP500/sp500_cleaned.csv\", \"sp500\", timeframe=tf\n",
    ")\n",
    "sp500_close_df = sp500_df[[\"sp500_close\"]]\n",
    "uk100_df = prepare_additional_data(\n",
    "    \"data/UK100/uk100_cleaned.csv\", \"uk100\", timeframe=tf\n",
    ")\n",
    "uk100_close_df = uk100_df[[\"uk100_close\"]]\n",
    "aus200_df = prepare_additional_data(\n",
    "    \"data/AUS200/aus200_cleaned.csv\", \"aus200\", timeframe=tf\n",
    ")\n",
    "aus200_close_df = aus200_df[[\"aus200_close\"]]\n",
    "chi50_df = prepare_additional_data(\n",
    "    \"data/CHI50/chi50_cleaned.csv\", \"chi50\", timeframe=tf\n",
    ")\n",
    "chi50_close_df = chi50_df[[\"chi50_close\"]]\n",
    "dollar_idx_df = prepare_additional_data(\n",
    "    \"data/DOLLAR_IDX/dollar_idx_cleaned.csv\", \"dollar_idx\", timeframe=tf\n",
    ")\n",
    "dollar_idx_close_df = dollar_idx_df[[\"dollar_idx_close\"]]\n",
    "eurbond_df = prepare_additional_data(\n",
    "    \"data/EUR_Bonds/eurbond_cleaned.csv\", \"eurbond\", timeframe=tf\n",
    ")\n",
    "eurbond_close_df = eurbond_df[[\"eurbond_close\"]]\n",
    "jpn225_df = prepare_additional_data(\n",
    "    \"data/JPN225/jpn225_cleaned.csv\", \"jpn225\", timeframe=tf\n",
    ")\n",
    "jpn225_close_df = jpn225_df[[\"jpn225_close\"]]\n",
    "ukbonds_df = prepare_additional_data(\n",
    "    \"data/UK_Bonds/ukbonds_cleaned.csv\", \"ukbonds\", timeframe=tf\n",
    ")\n",
    "ukbonds_close_df = ukbonds_df[[\"ukbonds_close\"]]\n",
    "ussc2000_df = prepare_additional_data(\n",
    "    \"data/USSC2000/ussc2000_cleaned.csv\", \"ussc2000\", timeframe=tf\n",
    ")\n",
    "ussc2000_close_df = ussc2000_df[[\"ussc2000_close\"]]\n",
    "\n",
    "# print(eurusd_df)\n",
    "# print(gbpusd_df)\n",
    "# print(xauusd_df)\n",
    "# print(xleusd_df)\n",
    "# print(xlpusd_df)\n",
    "# print(ustbond_df)\n",
    "# print(sp500_df)\n",
    "# print(uk100_df)\n",
    "# print(aus200_df)\n",
    "# print(chi50_df)\n",
    "# print(dollar_idx_df)\n",
    "# print(eurbond_df)\n",
    "# print(jpn225_df)\n",
    "# print(ukbonds_df)\n",
    "# print(ussc2000_df)\n",
    "\n",
    "# # Merge all into a single DataFrame\n",
    "# final_data = (\n",
    "#     data_close_df.join(ethusdt_close_df, how=\"left\")\n",
    "#     .join(bnbusdt_close_df, how=\"left\")\n",
    "#     .join(xrpusdt_close_df, how=\"left\")\n",
    "#     # .join(solusdt_close_df, how=\"left\")\n",
    "#     .join(adausdt_close_df, how=\"left\")\n",
    "#     # .join(dogeusdt_close_df, how=\"left\")\n",
    "#     .join(trxusdt_close_df, how=\"left\")\n",
    "#     # .join(avaxusdt_close_df, how=\"left\")\n",
    "#     # .join(shibusdt_close_df, how=\"left\")\n",
    "#     # .join(dotusdt_close_df, how=\"left\")\n",
    "#     .join(xlmusdt_close_df, how=\"left\")\n",
    "#     # .join(xmrusdt_close_df, how=\"left\")\n",
    "#     .join(neousdt_close_df, how=\"left\")\n",
    "#     .join(ltcusdt_close_df, how=\"left\")\n",
    "#     # .join(adausdt_close_df, how=\"left\")\n",
    "#     .join(eosusdt_close_df, how=\"left\")\n",
    "#     # .join(eurusd_close_df, how=\"left\")\n",
    "#     # .join(ustbond_close_df, how=\"left\")\n",
    "#     # .join(xauusd_close_df, how=\"left\")\n",
    "#     # .join(xleusd_close_df, how=\"left\")\n",
    "#     # .join(xlpusd_close_df, how=\"left\")\n",
    "#     # .join(sp500_close_df, how=\"left\")\n",
    "#     # .join(gbpusd_close_df, how=\"left\")\n",
    "#     # .join(uk100_close_df, how=\"left\")\n",
    "#     # .join(aus200_close_df, how=\"left\")\n",
    "#     # .join(chi50_close_df, how=\"left\")\n",
    "#     # .join(dollar_idx_close_df, how=\"left\")\n",
    "#     # .join(eurbond_close_df, how=\"left\")\n",
    "#     # .join(jpn225_close_df, how=\"left\")\n",
    "#     # .join(ukbonds_close_df, how=\"left\")\n",
    "#     # .join(ussc2000_close_df, how=\"left\")\n",
    "# )\n",
    "\n",
    "# Merge all into a single DataFrame\n",
    "final_data = (\n",
    "    data.join(ethusdt_df, how=\"left\")\n",
    "    .join(bnbusdt_df, how=\"left\")\n",
    "    .join(xrpusdt_df, how=\"left\")\n",
    "    # .join(solusdt_close_df, how=\"left\")\n",
    "    .join(adausdt_df, how=\"left\")\n",
    "    # .join(dogeusdt_close_df, how=\"left\")\n",
    "    .join(trxusdt_df, how=\"left\")\n",
    "    # .join(avaxusdt_close_df, how=\"left\")\n",
    "    # .join(shibusdt_close_df, how=\"left\")\n",
    "    # .join(dotusdt_close_df, how=\"left\")\n",
    "    .join(xlmusdt_df, how=\"left\")\n",
    "    # .join(xmrusdt_close_df, how=\"left\")\n",
    "    .join(neousdt_df, how=\"left\")\n",
    "    .join(ltcusdt_df, how=\"left\")\n",
    "    # .join(adausdt_close_df, how=\"left\")\n",
    "    .join(eosusdt_df, how=\"left\")\n",
    "    # .join(eurusd_close_df, how=\"left\")\n",
    "    # .join(ustbond_close_df, how=\"left\")\n",
    "    # .join(xauusd_close_df, how=\"left\")\n",
    "    # .join(xleusd_close_df, how=\"left\")\n",
    "    # .join(xlpusd_close_df, how=\"left\")\n",
    "    # .join(sp500_close_df, how=\"left\")\n",
    "    # .join(gbpusd_close_df, how=\"left\")\n",
    "    # .join(uk100_close_df, how=\"left\")\n",
    "    # .join(aus200_close_df, how=\"left\")\n",
    "    # .join(chi50_close_df, how=\"left\")\n",
    "    # .join(dollar_idx_close_df, how=\"left\")\n",
    "    # .join(eurbond_close_df, how=\"left\")\n",
    "    # .join(jpn225_close_df, how=\"left\")\n",
    "    # .join(ukbonds_close_df, how=\"left\")\n",
    "    # .join(ussc2000_close_df, how=\"left\")\n",
    ")\n",
    "\n",
    "final_data = final_data.dropna()\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# for tf in timeframes:\n",
    "#     dataframes[tf] = get_timeframe_data(symbol, from_time, to_time, tf)\n",
    "\n",
    "dataframes[tf] = final_data\n",
    "\n",
    "# # Syncronize the timeframes after computing the features\n",
    "# for tf in timeframes:\n",
    "#     dataframes[tf] = calculate_indicators(dataframes[tf]).dropna()\n",
    "\n",
    "latest_start_date = None\n",
    "earliest_end_date = None\n",
    "\n",
    "for df in dataframes.values():\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    if latest_start_date is None or start_date > latest_start_date:\n",
    "        latest_start_date = start_date\n",
    "    if earliest_end_date is None or end_date < earliest_end_date:\n",
    "        earliest_end_date = end_date\n",
    "\n",
    "# Ensure all DataFrames start and end on these dates\n",
    "for tf in dataframes:\n",
    "    dataframes[tf] = dataframes[tf][\n",
    "        (dataframes[tf].index >= latest_start_date)\n",
    "        & (dataframes[tf].index <= earliest_end_date)\n",
    "    ]\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")\n",
    "print(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     btcusdt_open  btcusdt_high  btcusdt_low  btcusdt_close  \\\n",
      "Date                                                                          \n",
      "2018-06-11 11:30:00       6761.28       6772.99      6761.28        6765.00   \n",
      "2018-06-11 11:31:00       6765.00       6765.85      6758.57        6760.00   \n",
      "2018-06-11 11:32:00       6760.00       6768.27      6760.00        6765.77   \n",
      "2018-06-11 11:33:00       6765.77       6765.77      6762.06        6765.75   \n",
      "2018-06-11 11:34:00       6765.76       6765.77      6764.69        6765.76   \n",
      "...                           ...           ...          ...            ...   \n",
      "2025-02-07 22:56:00      95821.50      95826.00     95821.40       95825.30   \n",
      "2025-02-07 22:57:00      95825.30      95891.20     95825.20       95891.20   \n",
      "2025-02-07 22:58:00      95892.00      96058.00     95892.00       96008.60   \n",
      "2025-02-07 22:59:00      96008.10      96024.00     95975.20       96014.30   \n",
      "2025-02-07 23:00:00      96014.20      96014.30     95861.10       95877.90   \n",
      "\n",
      "                     btcusdt_volume  ethusdt_open  ethusdt_high  ethusdt_low  \\\n",
      "Date                                                                           \n",
      "2018-06-11 11:30:00        3.433960        531.93        532.45       531.39   \n",
      "2018-06-11 11:31:00       11.539627        531.65        531.80       531.39   \n",
      "2018-06-11 11:32:00       10.598203        531.65        531.77       531.06   \n",
      "2018-06-11 11:33:00        2.724804        531.62        532.12       531.48   \n",
      "2018-06-11 11:34:00        6.028348        531.74        532.15       531.50   \n",
      "...                             ...           ...           ...          ...   \n",
      "2025-02-07 22:56:00       22.834000       2577.43       2578.67      2576.99   \n",
      "2025-02-07 22:57:00       67.510000       2578.67       2581.00      2577.91   \n",
      "2025-02-07 22:58:00      123.523000       2580.57       2586.00      2580.57   \n",
      "2025-02-07 22:59:00       99.044000       2584.50       2584.76      2582.23   \n",
      "2025-02-07 23:00:00      121.443000       2582.24       2582.24      2576.81   \n",
      "\n",
      "                     ethusdt_close  ethusdt_volume  ...  ltcusdt_open  \\\n",
      "Date                                                ...                 \n",
      "2018-06-11 11:30:00         531.85        93.16106  ...        106.76   \n",
      "2018-06-11 11:31:00         531.65        75.65343  ...        106.78   \n",
      "2018-06-11 11:32:00         531.77        45.77927  ...        106.89   \n",
      "2018-06-11 11:33:00         531.89        26.13004  ...        106.84   \n",
      "2018-06-11 11:34:00         531.89        58.14813  ...        106.90   \n",
      "...                            ...             ...  ...           ...   \n",
      "2025-02-07 22:56:00        2578.66      1810.59500  ...        101.34   \n",
      "2025-02-07 22:57:00        2580.56      2222.81500  ...        101.34   \n",
      "2025-02-07 22:58:00        2584.51      3745.46600  ...        101.43   \n",
      "2025-02-07 22:59:00        2582.23      2539.10900  ...        101.63   \n",
      "2025-02-07 23:00:00        2579.73      3367.53400  ...        101.66   \n",
      "\n",
      "                     ltcusdt_high  ltcusdt_low  ltcusdt_close  ltcusdt_volume  \\\n",
      "Date                                                                            \n",
      "2018-06-11 11:30:00        106.80       106.72         106.78        15.18664   \n",
      "2018-06-11 11:31:00        106.78       106.78         106.78        10.33512   \n",
      "2018-06-11 11:32:00        106.89       106.84         106.84         0.13100   \n",
      "2018-06-11 11:33:00        106.90       106.84         106.90        21.00000   \n",
      "2018-06-11 11:34:00        106.90       106.90         106.90         5.77448   \n",
      "...                           ...          ...            ...             ...   \n",
      "2025-02-07 22:56:00        101.37       101.26         101.33      1041.07500   \n",
      "2025-02-07 22:57:00        101.43       101.34         101.43       452.25500   \n",
      "2025-02-07 22:58:00        101.69       101.43         101.64      2332.51900   \n",
      "2025-02-07 22:59:00        101.68       101.60         101.67      1190.55100   \n",
      "2025-02-07 23:00:00        101.67       101.39         101.46      2058.32700   \n",
      "\n",
      "                     eosusdt_open  eosusdt_high  eosusdt_low  eosusdt_close  \\\n",
      "Date                                                                          \n",
      "2018-06-11 11:30:00       11.3401       11.3466      11.3097        11.3338   \n",
      "2018-06-11 11:31:00       11.3347       11.3356      11.3204        11.3206   \n",
      "2018-06-11 11:32:00       11.3364       11.3438      11.3268        11.3438   \n",
      "2018-06-11 11:33:00       11.3438       11.3600      11.3332        11.3593   \n",
      "2018-06-11 11:34:00       11.3593       11.3641      11.3448        11.3641   \n",
      "...                           ...           ...          ...            ...   \n",
      "2025-02-07 22:56:00        0.5730        0.5740       0.5720         0.5740   \n",
      "2025-02-07 22:57:00        0.5730        0.5740       0.5730         0.5740   \n",
      "2025-02-07 22:58:00        0.5740        0.5770       0.5730         0.5760   \n",
      "2025-02-07 22:59:00        0.5760        0.5760       0.5750         0.5750   \n",
      "2025-02-07 23:00:00        0.5740        0.5750       0.5730         0.5730   \n",
      "\n",
      "                     eosusdt_volume  \n",
      "Date                                 \n",
      "2018-06-11 11:30:00          943.79  \n",
      "2018-06-11 11:31:00         9782.28  \n",
      "2018-06-11 11:32:00         1229.61  \n",
      "2018-06-11 11:33:00         2784.00  \n",
      "2018-06-11 11:34:00         6153.99  \n",
      "...                             ...  \n",
      "2025-02-07 22:56:00        81046.10  \n",
      "2025-02-07 22:57:00        24860.50  \n",
      "2025-02-07 22:58:00       131078.40  \n",
      "2025-02-07 22:59:00        89000.80  \n",
      "2025-02-07 23:00:00        40708.70  \n",
      "\n",
      "[3500594 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# Align lengths by truncating to the shortest length after resampling\n",
    "min_length = min(len(df) for df in dataframes.values())\n",
    "aligned_data = {\n",
    "    tf: df.iloc[:min_length]\n",
    "    for tf, df in dataframes.items()\n",
    "}\n",
    "\n",
    "# Concatenate data from all timeframes\n",
    "concatenated_data = pd.concat(aligned_data.values(), axis=1)\n",
    "concatenated_data.index.name = \"Date\"\n",
    "concatenated_data.to_csv(\"Diversified_Portfolio_Data_Complete_DRL.csv\")\n",
    "\n",
    "print(concatenated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
